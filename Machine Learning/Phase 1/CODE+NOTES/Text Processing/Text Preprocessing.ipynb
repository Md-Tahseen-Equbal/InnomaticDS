{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cca50af-c74c-432e-803e-f17acec22264",
   "metadata": {},
   "source": [
    "## NLP Pipeline\n",
    "‚û°Ô∏è **Text** ‚û°Ô∏è **Text Preprocessing** ‚û°Ô∏è **Feature Extraction/ Feature vectorization** ‚û°Ô∏è **Structured & Numerical Data**\n",
    "\n",
    "### Text Preprocessing Challenges\n",
    "- Lowercase\n",
    "- Spelling Mistake/ Correction\n",
    "- Emoji Prediction\n",
    "- Html tags\n",
    "- URL's\n",
    "- Chat Words (OMG, ASAP...etc)\n",
    "- Puntuations\n",
    "- Stop Words\n",
    "- Tokenization\n",
    "- Stemming\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c4b461-b893-432a-90c4-79b77b8b43b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I am Data Scientist.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3006be89-6db9-4047-aba0-05e9288c13fe",
   "metadata": {},
   "source": [
    "### Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74412481-7cee-4933-b664-d36d2233e99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am data scientist.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a394d8c7-938f-41f1-a3eb-4df411799a7b",
   "metadata": {},
   "source": [
    "### Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11a794f3-464d-4af3-8780-0353d5c3258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'I am Dala Scintistey.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ce1cd8-57f5-4f07-b7a3-bc0a2d2fd207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af65a3ee-b849-4e6f-933a-47c46b2cf505",
   "metadata": {},
   "outputs": [],
   "source": [
    "speller = autocorrect.Speller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f197413-f1e7-4bb3-8722-566173557860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am Data Scientist.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speller.autocorrect_sentence(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4009f15-606d-4ae5-aa9a-f10d17f5c4fb",
   "metadata": {},
   "source": [
    "### Emoji Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d9b4620-869f-4fb7-ac10-9e74b5bafa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Biryani is Tastyüòã\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b672d016-4d71-452a-828b-9fe6c16a6d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f0da815-3855-4986-b1fb-eba7f470e5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Biryani is Tasty face_savoring_food '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji.demojize(text).replace(\":\",' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec01dd8-448c-44c0-9750-c9d409e68110",
   "metadata": {},
   "source": [
    "### Chat Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "506e3025-5dbe-4a43-a694-10dffd5c666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = {\n",
    "    \"brb\": \"be right back\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"idk\": \"I don't know\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"imho\": \"in my humble opinion\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"wbu\": \"what about you\",\n",
    "    \"bc\": \"because\",\n",
    "    \"afaik\": \"as far as I know\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"atm\": \"at the moment\",\n",
    "    \"bbl\": \"be back later\",\n",
    "    \"bfn\": \"bye for now\",\n",
    "    \"bff\": \"best friends forever\",\n",
    "    \"cu\": \"see you\",\n",
    "    \"cya\": \"see you\",\n",
    "    \"dm\": \"direct message\",\n",
    "    \"fb\": \"Facebook\",\n",
    "    \"ftw\": \"for the win\",\n",
    "    \"gg\": \"good game\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"gtg\": \"got to go\",\n",
    "    \"hbu\": \"how about you\",\n",
    "    \"ily\": \"I love you\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    \"lmao\": \"laughing my ass off\",\n",
    "    \"lmk\": \"let me know\",\n",
    "    \"nvm\": \"never mind\",\n",
    "    \"omw\": \"on my way\",\n",
    "    \"plz\": \"please\",\n",
    "    \"ppl\": \"people\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"u\": \"you\",\n",
    "    \"ur\": \"your\",\n",
    "    \"yolo\": \"you only live once\",\n",
    "    \"yw\": \"you're welcome\",\n",
    "    \"ty\": \"thank you\",\n",
    "    \"abt\" : \"about\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66e53bdc-764d-4650-ae2c-5c7dfe3a95b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"omg idk abt it ty so much.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e06da6f-ae62-4410-a184-9bb9406c6523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"oh my god I don't know about it thank you so much.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in text.split():\n",
    "    if word in chat.keys():\n",
    "        text = text.replace(word,chat[word])\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc08e9e-442f-4780-955d-ab7daed59f17",
   "metadata": {},
   "source": [
    "### HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62dbda3f-fc15-4f68-98bc-67c65bb9b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Sample Text Data</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Welcome to the Sample Text Data</h1>\n",
    "    <p>This is a <strong>paragraph</strong> with some <em>HTML</em> tags for <a href=\"https://example.com\">linking</a>.</p>\n",
    "    <div>\n",
    "        <p>Another paragraph in a <code>&lt;div&gt;</code> element.</p>\n",
    "        <ul>\n",
    "            <li>List item one</li>\n",
    "            <li>List item two</li>\n",
    "            <li>List item three</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    <footer>\n",
    "        <p>Contact us at <a href=\"mailto:example@example.com\">example@example.com</a></p>\n",
    "    </footer>\n",
    "</body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0acaa72f-94c5-4ad5-bd1b-12e85a77131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'<[^>]+>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23ebf9aa-e600-476c-900b-414a320373cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efef9822-7693-4212-b42f-fc46257268d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "     Sample Text Data \n",
      " \n",
      " \n",
      "     Welcome to the Sample Text Data \n",
      "     This is a  paragraph  with some  HTML  tags for  linking . \n",
      "     \n",
      "         Another paragraph in a  &lt;div&gt;  element. \n",
      "         \n",
      "             List item one \n",
      "             List item two \n",
      "             List item three \n",
      "         \n",
      "     \n",
      "     \n",
      "         Contact us at  example@example.com  \n",
      "     \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(pattern,\" \",text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71128bf8-4091-4878-9b3d-2d6c9f07d898",
   "metadata": {},
   "source": [
    "### URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05c47b01-229d-4dfc-945e-52efd10b7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Check out the latest updates on technology at TechCrunch https://techcrunch.com.\n",
    "\n",
    "For coding tutorials and resources, visit freeCodeCamp https://www.freecodecamp.org.\n",
    "\n",
    "If you are looking for academic research papers, Google Scholar https://scholar.google.com is a great resource.\n",
    "\n",
    "For news and cu'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07000d80-b9c0-45ef-82a6-d7b888ab0e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'www.\\S+ |https?:\\S+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0b11dd1-514c-4b00-9867-6c8a89b67425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out the latest updates on technology at TechCrunch \n",
      "\n",
      "For coding tutorials and resources, visit freeCodeCamp \n",
      "\n",
      "If you are looking for academic research papers, Google Scholar  is a great resource.\n",
      "\n",
      "For news and cu\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(pattern,\"\",text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2132e63c-acb5-45eb-976f-bc44ba24515a",
   "metadata": {},
   "source": [
    "### Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "513b4d2c-5ccd-403f-bf10-78e70286a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"i'm data scientist, data analyst and ML enginering. and you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f366fe81-8fcc-4461-aa43-f410e04b74be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'[^\\w]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce3f13a1-d374-4d37-b744-1c62bfd745a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i m data scientist  data analyst and ML enginering  and you '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(pattern,\" \",text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a645e08f-24c5-428f-b7a1-5da5a7d7d7fd",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e00410d-d4fc-4242-a869-3e955255c152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77738d74-15c3-492f-b27d-8b6a82e95337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b86ddd6c-fb14-4186-9442-c5f01d38ec84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91830\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1f11020-77e0-4031-b0cb-ac31594880e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"i am data scientist  data analyst and ML enginering  and you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "404acb75-e758-4dde-a709-596049f3f6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93f5bf12-b806-4f8b-bd6e-72965e5d429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = []\n",
    "for i in text.split():\n",
    "    if i not in stopwords.words('english'):\n",
    "        doc.append(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97ee225d-b494-4130-95e9-0e4d69b8fa11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data scientist data analyst ML enginering'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592bd74a-2dc0-448e-b99a-c348235afc67",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "- Tokenization is used to convert document to words(tokens) / sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e91cac7-c83c-4196-8687-6fcc9b548ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am data analyst. I am Engineer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "137c4f8c-c4ec-4fce-a9c6-04afb9860e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b23f572-57cf-4734-a3ef-e616c00d7e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91830\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51af82a4-2c1e-4e54-b4f7-d0e03195c1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am data analyst.', 'I am Engineer.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1d47ee5-b60e-4534-994d-e4bdbafd0be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'data', 'analyst', '.', 'I', 'am', 'Engineer', '.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5baf9db-3f9d-4657-8ca6-0fb36e7c7fdc",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "- Stemming is the process of converting tokenized words into it's base form / root word by removing the suffixes.\n",
    "- **Types of stemming :**\n",
    "  1. Porter Stemmer\n",
    "     - English language\n",
    "  2. Lancaster Stemmer\n",
    "     - English Language\n",
    "     - more Agressive (It removes the more suffiex words)\n",
    "  3. Snowball Stemmer\n",
    "     - Multiple Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8c7d15f-f032-445f-b6ac-55d4118a1d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44844042-12db-4c16-bc97-e97f160d412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "port = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bdbe4994-1409-4a1a-9f75-cbf6d89f4444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'come'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port.stem(\"coming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2544242f-8caa-49e5-ae7a-7d461d720747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goe'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port.stem(\"goes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41400964-46ed-40f4-97a9-0087188a4f18",
   "metadata": {},
   "source": [
    "## Lematization\n",
    "- The process of converting the word to the dictionary form/ Lemma.\n",
    "- It is slower when compared to stemming.\n",
    "- It is accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab9337ba-b320-4ea2-bb07-4b34b5579080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2814507-5601-428f-8da4-056f6a9fbe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28163a8d-a6ac-402f-8295-539a69f2d984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91830\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a507f7d-9bd5-4b52-aed9-454279d8b5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'come'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize(\"comes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2b551008-362c-4d35-85a3-57ac906cc3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize(\"goes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b5cd6-0751-4d4b-875a-05783e53c34c",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f640c7d-1b2f-450e-a327-c27c73679471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5244a3ff-d31e-4344-a027-3e98c5a65d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\91830\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8f0eb317-6569-42e6-a036-77677991176d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'), ('am', 'VBP'), ('data', 'NNS')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"i\",\"am\",\"data\"]\n",
    "pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbc3088-b175-4eed-ba73-0fefbcd5a1df",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "251e2d4f-a237-4a7b-a52e-243d40726b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autocorrect\n",
    "from nltk.stem import PorterStemmer\n",
    "import emoji\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7dd1c5b7-07a9-44e2-b0ec-0aa29f8f4f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    speller = autocorrect.Speller()\n",
    "    stem = PorterStemmer()\n",
    "    lemma = WordNetLemmatizer()\n",
    "    \n",
    "    text = text.lower() # Converting to lowercase for uniformity\n",
    "    text = speller.autocorrect_sentence(text) # correcting the spelling mistakes\n",
    "    text = emoji.demojize(text).replace(':',' ') # emoji prediction and converting to text\n",
    "    text = re.sub(r'www.\\S+|https?://\\S+',' ',text) # rerplacing the urls\n",
    "    text = re.sub(r'<[^>]+>',\" \",text) # Html tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9']\",' ',text) # removing the puntuations\n",
    "    text = re.sub(r'[0-9] ','',text) # replacing the numbers\n",
    "    text = text = ' '.join(map(lambda i: chat[i] if i in chat.keys() else i, text.split())) # caht words \n",
    "    text = word_tokenize(text) # word tokenization\n",
    "    text = [stem.stem(i) for i in text] #stemming\n",
    "    text = [lemma.lemmatize(i)for i in text] # LEmatization\n",
    "    text = [i for i in text if i not in stopwords.words('english') ] # stop words remover\n",
    "   \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "89ef6fdf-f41e-45b1-a6f5-85ab93f0a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81026e97-e4ce-45cd-a4c2-42268d78a097",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\91830\\Downloads\\Data Science  Course\\Machine Learning\\Text Processing (NLP)\\sample_text_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d3608b90-ef53-4f81-8ec7-71ed3118d03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>OMG! I can not believe it! üòÇüòÇ #unbelievable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I just had the best pizza ever!!! üçïüçï @BestPizz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Weather today is so nice, totally loving it. ‚òÄÔ∏èüå¥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Ugh, Monday mornings are the worst üò©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Just finished a 10k run, feeling great! #fitness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Going to the movies tonight. Any recommendatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Why is this happening to me? üò¢</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>I love my new phone! üì± So fast and sleek.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Had a great time at the beach today with frien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>I can not wait for the weekend! üéâüéâ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text\n",
       "0   1        OMG! I can not believe it! üòÇüòÇ #unbelievable\n",
       "1   2  I just had the best pizza ever!!! üçïüçï @BestPizz...\n",
       "2   3   Weather today is so nice, totally loving it. ‚òÄÔ∏èüå¥\n",
       "3   4               Ugh, Monday mornings are the worst üò©\n",
       "4   5   Just finished a 10k run, feeling great! #fitness\n",
       "5   6  Going to the movies tonight. Any recommendatio...\n",
       "6   7                     Why is this happening to me? üò¢\n",
       "7   8          I love my new phone! üì± So fast and sleek.\n",
       "8   9  Had a great time at the beach today with frien...\n",
       "9  10                 I can not wait for the weekend! üéâüéâ"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "52fe5fad-7f9f-4b67-9df6-36a597f81352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [oh, god, believ, face, tear, joy, face, tear,...\n",
       "1     [best, pizza, ever, pizza, pizza, bestpizzaplac]\n",
       "2    [weather, today, nice, total, love, sun, palm,...\n",
       "3               [gh, monday, morn, worst, weari, face]\n",
       "4                 [finish, 10k, run, feel, great, fit]\n",
       "5    [go, movi, tonight, ani, recommend, clapper, b...\n",
       "6                        [whi, thi, happen, cri, face]\n",
       "7        [love, new, phone, mobil, phone, fast, sleek]\n",
       "8    [great, time, beach, today, friend, beach, umb...\n",
       "9        [wait, weekend, parti, popper, parti, popper]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].apply(text_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74863919-b3f7-450e-aae3-a272046afaf4",
   "metadata": {},
   "source": [
    "# Feature Vectorization\n",
    "- Feature Vectorization is used to convert the preprocesed text to numerical.\n",
    "- **Types of Feature Vectorization**\n",
    "  1. Heuristic\n",
    "     - re\n",
    "     - wordnet\n",
    "  2. ML\n",
    "     - One hot Encoding\n",
    "     - Index Based\n",
    "     - BOW(Bag Of Words)\n",
    "     - TF-IDF\n",
    "  3. DL Approch\n",
    "     - wordZvec\n",
    "     - Fast text\n",
    "     - gpt\n",
    "     - llms ....... etc\n",
    "## One Hot Encoding\n",
    "- OHE is the binary representation of feature vectorization.\n",
    "-  0 -> Absent  |   1 -> Present\n",
    "-  words -> Columns | document -> Rows\n",
    "-  **Steps to implement OHE:**\n",
    "      1. Preprocessing text.\n",
    "      2. Tokenize the text data.\n",
    "      3. Create vocabulary.\n",
    "      4. Assign the binary values to the words  0 -> Absent  |   1 -> Present.\n",
    "#### Pros / Con's for applying OHE\n",
    "- **PROS**\n",
    "  - Intuitive\n",
    "  - Simple to understand.\n",
    "- **Con's**\n",
    "  - Input shape is not constant.\n",
    "    - Based on vocabulary, th i/p shape gets changed. \n",
    "  - Order/ sequence is missing.\n",
    "    - Contextual meaning is missed.\n",
    "  - Sparse matrix.\n",
    "    - The data which contains more zero's. (The opposite of spares matrix is **Dense Matrix**, means less Zero's.)\n",
    "    - Multi Colinearity(almost same data in both columns it meeans it has strong relationship.)\n",
    "    - ML can't capture more pattrens.\n",
    "  - OOV(order of vocabulary).\n",
    "    - Outside of set of unique words.\n",
    "    - During training ML algorithm trained on vocabulary of test data, during testing if a new word comes which is not present in the vocabulary, Ml cant understand that word.\n",
    "  - Lack of semmanting.\n",
    "    - The relationship between words.\n",
    "\n",
    "## Index Based Encoding\n",
    "- The idea behind the index-based encoding is to map each word with one index, i.e., a number. The first step is to create a dictionary that maps words to indexes.\n",
    "- **PROS**\n",
    "  - Intuitive\n",
    "  - Simple to understand.\n",
    "  - Present contextual meaning.\n",
    "- **Con's**\n",
    "    - Input shape is not constant.\n",
    "    - Order/ sequence is missing.\n",
    "    - Sparse matrix.\n",
    "    - OOV(order of vocabulary).\n",
    "    - Lack of semmanting.\n",
    " \n",
    "## BOW(Bag Of Words)\n",
    "- BOw is used to convert preprocessed to structural & numerical.\n",
    "- BOW deals with the representation of <ins>count</ins> ie. no.of ocurences of word in the <ins>document(row/cell)</ins>.\n",
    "- BOW gives the importance to words in the document.\n",
    "-  **Steps to implement BOW:**\n",
    "      1. Preprocessing text.\n",
    "      2. Tokenize the text data.\n",
    "      3. Create vocabulary.\n",
    "      4. Assign the values based upon no of occurences.\n",
    "- **PROS**\n",
    "  - Intuitive\n",
    "  - Simple to understand.\n",
    "  - Importance to words.\n",
    "- **Con's**\n",
    "    - Input shape is not constant.\n",
    "      - Solution: max_features\n",
    "    - Order/ sequence is missing.\n",
    "      - Solution: N grams.\n",
    "    - Sparse matrix.\n",
    "    - OOV(order of vocabulary).\n",
    "    - Lack of semmanting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb072fb8-12b3-449c-bca9-57e04995ebd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
